#!/usr/bin/env python3
"""
Demonstra√ß√£o do Sistema de Websearch para Stratus.IA
Testa todos os componentes: Search Engine, Scraper, Validator e Knowledge Updater
"""

import asyncio
import os
import sys
from datetime import datetime
from typing import List

# Adiciona o diret√≥rio src ao path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from src.websearch.engine import StratusWebSearchEngine
from src.websearch.scraper import StratusContentScraper, ScrapingConfig
from src.websearch.validator import StratusSearchValidator, ValidationConfig
from src.websearch.updater import StratusKnowledgeUpdater, UpdateConfig
from src.websearch.base import SearchDomain, ContentType, SourceReliability
from src.utils.logging import get_logger


class WebSearchDemo:
    """Demonstra√ß√£o completa do sistema de websearch"""
    
    def __init__(self):
        self.logger = get_logger()
        
        # Inicializa componentes
        self.search_engine = StratusWebSearchEngine()
        self.scraper = StratusContentScraper()
        self.validator = StratusSearchValidator()
        self.knowledge_updater = StratusKnowledgeUpdater()
        
        # Queries de teste
        self.test_queries = [
            "METAR SBGR",
            "NOTAM SBSP",
            "RBAC 91",
            "emerg√™ncia aeron√°utica",
            "DECEA meteorologia",
            "ANAC regulamentos",
            "ICAO procedimentos",
            "aeroporto Congonhas",
            "pista 09L SBGR",
            "visibilidade tempo"
        ]
        
        # URLs de teste para scraping
        self.test_urls = [
            "https://www.anac.gov.br",
            "https://www.decea.gov.br",
            "https://www.icao.int",
            "https://www.faa.gov",
            "https://www.easa.europa.eu"
        ]
    
    async def run_demo(self):
        """Executa demonstra√ß√£o completa"""
        self.logger._log_info("üöÄ Iniciando demonstra√ß√£o do Sistema de Websearch")
        self.logger._log_info("=" * 60)
        
        try:
            # 1. Teste do Web Search Engine
            await self.demo_search_engine()
            
            # 2. Teste do Content Scraper
            await self.demo_content_scraper()
            
            # 3. Teste do Search Result Validator
            await self.demo_search_validator()
            
            # 4. Teste do Knowledge Updater
            await self.demo_knowledge_updater()
            
            # 5. Teste integrado
            await self.demo_integrated_workflow()
            
            # 6. Exibe m√©tricas finais
            self.show_final_metrics()
            
        except Exception as e:
            self.logger._log_error(f"Erro na demonstra√ß√£o: {str(e)}")
            raise
    
    async def demo_search_engine(self):
        """Demonstra o Web Search Engine"""
        self.logger._log_info("üîç Testando Web Search Engine")
        self.logger._log_info("-" * 40)
        
        for i, query in enumerate(self.test_queries[:3], 1):
            self.logger._log_info(f"Query {i}: {query}")
            
            try:
                # Executa busca
                results = await self.search_engine.search(
                    query=query,
                    max_results=5,
                    force_fresh=True
                )
                
                self.logger._log_info(f"  ‚úÖ Resultados encontrados: {len(results)}")
                
                # Exibe primeiros resultados
                for j, result in enumerate(results[:2], 1):
                    self.logger._log_info(f"    {j}. {result.title[:50]}...")
                    self.logger._log_info(f"       URL: {result.url}")
                    self.logger._log_info(f"       Tipo: {result.content_type.value}")
                    self.logger._log_info(f"       Confiabilidade: {result.source_reliability.value}")
                    self.logger._log_info(f"       Score: {result.relevance_score:.2f}")
                
                await asyncio.sleep(1)  # Rate limiting
                
            except Exception as e:
                self.logger._log_error(f"  ‚ùå Erro na busca: {str(e)}")
        
        # Exibe m√©tricas do search engine
        metrics = self.search_engine.get_metrics()
        self.logger._log_info(f"üìä M√©tricas do Search Engine:")
        self.logger._log_info(f"   Total de buscas: {metrics.total_searches}")
        self.logger._log_info(f"   Taxa de sucesso: {metrics.successful_searches}/{metrics.total_searches}")
        self.logger._log_info(f"   Tempo m√©dio: {metrics.avg_response_time:.2f}s")
        self.logger._log_info(f"   Taxa de cache: {metrics.cache_hit_rate:.2f}")
    
    async def demo_content_scraper(self):
        """Demonstra o Content Scraper"""
        self.logger._log_info("\nüìÑ Testando Content Scraper")
        self.logger._log_info("-" * 40)
        
        for i, url in enumerate(self.test_urls[:3], 1):
            self.logger._log_info(f"URL {i}: {url}")
            
            try:
                # Executa scraping
                scraped_content = await self.scraper.scrape_content(
                    url=url,
                    force_fresh=True
                )
                
                if scraped_content and scraped_content.status.value == "SUCCESS":
                    self.logger._log_info(f"  ‚úÖ Conte√∫do extra√≠do: {len(scraped_content.content)} caracteres")
                    self.logger._log_info(f"     T√≠tulo: {scraped_content.title[:50]}...")
                    self.logger._log_info(f"     Tipo: {scraped_content.content_type.value}")
                    self.logger._log_info(f"     Confiabilidade: {scraped_content.source_reliability.value}")
                    
                    # Exibe dados estruturados se dispon√≠veis
                    if scraped_content.structured_data:
                        self.logger._log_info(f"     Dados estruturados: {len(scraped_content.structured_data)} campos")
                else:
                    self.logger._log_warning(f"  ‚ö†Ô∏è Falha no scraping: {scraped_content.status.value if scraped_content else 'N/A'}")
                
                await asyncio.sleep(2)  # Rate limiting
                
            except Exception as e:
                self.logger._log_error(f"  ‚ùå Erro no scraping: {str(e)}")
        
        # Exibe m√©tricas do scraper
        metrics = self.scraper.get_metrics()
        self.logger._log_info(f"üìä M√©tricas do Scraper:")
        self.logger._log_info(f"   Total de scrapes: {metrics.total_scrapes}")
        self.logger._log_info(f"   Taxa de sucesso: {metrics.successful_scrapes}/{metrics.total_scrapes}")
        self.logger._log_info(f"   Tempo m√©dio: {metrics.avg_execution_time:.2f}s")
        self.logger._log_info(f"   Taxa de cache: {metrics.cache_hit_rate:.2f}")
    
    async def demo_search_validator(self):
        """Demonstra o Search Result Validator"""
        self.logger._log_info("\n‚úÖ Testando Search Result Validator")
        self.logger._log_info("-" * 40)
        
        # Cria resultados simulados para teste
        from src.websearch.base import SearchResult
        
        test_results = [
            SearchResult(
                url="https://www.anac.gov.br/notam",
                title="NOTAM - Avisos aos Aeronavegantes",
                snippet="Informa√ß√µes sobre NOTAMs e avisos importantes para pilotos",
                content="Conte√∫do detalhado sobre NOTAMs...",
                source_reliability=SourceReliability.OFFICIAL,
                content_type=ContentType.NOTAM,
                relevance_score=0.9,
                freshness_score=0.8,
                authority_score=1.0,
                extracted_data={"icao_codes": ["SBGR", "SBSP"], "notam_id": ["A1234/23"]}
            ),
            SearchResult(
                url="https://www.decea.gov.br/metar",
                title="METAR SBGR - Aeroporto de Congonhas",
                snippet="Condi√ß√µes meteorol√≥gicas atuais do aeroporto",
                content="METAR SBGR 261200Z 08008KT 9999 FEW020 SCT100 25/18 Q1018",
                source_reliability=SourceReliability.OFFICIAL,
                content_type=ContentType.METAR_TAF,
                relevance_score=0.8,
                freshness_score=0.9,
                authority_score=1.0,
                extracted_data={"icao_codes": ["SBGR"], "metar_data": ["METAR SBGR 261200Z 08008KT 9999 FEW020 SCT100 25/18 Q1018"]}
            ),
            SearchResult(
                url="https://spam-site.com/fake",
                title="Fake Aviation News",
                snippet="Conte√∫do suspeito e n√£o confi√°vel",
                content="Spam content...",
                source_reliability=SourceReliability.UNRELIABLE,
                content_type=ContentType.GENERAL,
                relevance_score=0.2,
                freshness_score=0.1,
                authority_score=0.1,
                extracted_data={}
            )
        ]
        
        for i, result in enumerate(test_results, 1):
            self.logger._log_info(f"Resultado {i}: {result.title[:30]}...")
            
            try:
                # Valida resultado
                validation = await self.validator.validate_result(result)
                
                self.logger._log_info(f"  ‚úÖ V√°lido: {validation.is_valid}")
                self.logger._log_info(f"     Score: {validation.validation_score:.2f}")
                self.logger._log_info(f"     Status: {validation.validation_status.value}")
                self.logger._log_info(f"     Confian√ßa: {validation.confidence_score:.2f}")
                
                if validation.validation_errors:
                    self.logger._log_info(f"     Erros: {validation.validation_errors}")
                if validation.validation_warnings:
                    self.logger._log_info(f"     Avisos: {validation.validation_warnings}")
                
            except Exception as e:
                self.logger._log_error(f"  ‚ùå Erro na valida√ß√£o: {str(e)}")
        
        # Exibe m√©tricas do validator
        metrics = self.validator.get_metrics()
        self.logger._log_info(f"üìä M√©tricas do Validator:")
        self.logger._log_info(f"   Total de valida√ß√µes: {metrics.total_validations}")
        self.logger._log_info(f"   Resultados v√°lidos: {metrics.valid_results}")
        self.logger._log_info(f"   Resultados inv√°lidos: {metrics.invalid_results}")
        self.logger._log_info(f"   Tempo m√©dio: {metrics.avg_execution_time:.2f}s")
    
    async def demo_knowledge_updater(self):
        """Demonstra o Knowledge Updater"""
        self.logger._log_info("\nüîÑ Testando Knowledge Updater")
        self.logger._log_info("-" * 40)
        
        # Cria resultados simulados para atualiza√ß√£o
        from src.websearch.base import SearchResult
        
        test_results = [
            SearchResult(
                url="https://www.anac.gov.br/rbac91",
                title="RBAC 91 - Regulamento Brasileiro de Avia√ß√£o Civil",
                snippet="Regulamento que estabelece as regras gerais de voo",
                content="Conte√∫do detalhado do RBAC 91...",
                source_reliability=SourceReliability.OFFICIAL,
                content_type=ContentType.REGULATION,
                relevance_score=0.9,
                freshness_score=0.7,
                authority_score=1.0,
                extracted_data={"rbac_references": ["RBAC 91"], "regulation_numbers": ["91/001"]}
            ),
            SearchResult(
                url="https://www.decea.gov.br/emergency",
                title="Procedimentos de Emerg√™ncia Aeron√°utica",
                snippet="Protocolos para situa√ß√µes de emerg√™ncia em voo",
                content="Procedimentos detalhados para emerg√™ncias...",
                source_reliability=SourceReliability.OFFICIAL,
                content_type=ContentType.EMERGENCY,
                relevance_score=0.95,
                freshness_score=0.9,
                authority_score=1.0,
                extracted_data={"emergency_keywords": ["EMERGENCY", "MAYDAY"], "priority_levels": ["CR√çTICO"]}
            )
        ]
        
        try:
            # Processa resultados para atualiza√ß√£o
            updates = await self.knowledge_updater.process_search_results(
                results=test_results,
                force_update=True
            )
            
            self.logger._log_info(f"  ‚úÖ Atualiza√ß√µes criadas: {len(updates)}")
            
            for i, update in enumerate(updates, 1):
                self.logger._log_info(f"    {i}. {update.content_type.value} - Prioridade: {update.priority}")
                self.logger._log_info(f"       ID: {update.id[:8]}...")
                self.logger._log_info(f"       Status: {update.status.value}")
            
            # Executa atualiza√ß√µes
            if updates:
                executed_updates = await self.knowledge_updater.execute_updates(updates)
                self.logger._log_info(f"  ‚úÖ Atualiza√ß√µes executadas: {len(executed_updates)}")
                
                for update in executed_updates:
                    self.logger._log_info(f"    - {update.id[:8]}... -> {update.status.value}")
            
        except Exception as e:
            self.logger._log_error(f"  ‚ùå Erro no knowledge updater: {str(e)}")
        
        # Exibe m√©tricas do knowledge updater
        metrics = self.knowledge_updater.get_metrics()
        self.logger._log_info(f"üìä M√©tricas do Knowledge Updater:")
        self.logger._log_info(f"   Total de atualiza√ß√µes: {metrics.total_updates}")
        self.logger._log_info(f"   Atualiza√ß√µes bem-sucedidas: {metrics.successful_updates}")
        self.logger._log_info(f"   Taxa de sucesso: {metrics.success_rate:.2f}")
    
    async def demo_integrated_workflow(self):
        """Demonstra workflow integrado"""
        self.logger._log_info("\nüîÑ Testando Workflow Integrado")
        self.logger._log_info("-" * 40)
        
        query = "METAR SBGR tempo atual"
        self.logger._log_info(f"Query: {query}")
        
        try:
            # 1. Busca
            self.logger._log_info("1Ô∏è‚É£ Executando busca...")
            search_results = await self.search_engine.search(query, max_results=3)
            self.logger._log_info(f"   Resultados encontrados: {len(search_results)}")
            
            if not search_results:
                self.logger._log_warning("   Nenhum resultado encontrado")
                return
            
            # 2. Valida√ß√£o
            self.logger._log_info("2Ô∏è‚É£ Validando resultados...")
            validation_results = await self.validator.validate_multiple_results(search_results)
            valid_results = [vr.search_result for vr in validation_results if vr.is_valid]
            self.logger._log_info(f"   Resultados v√°lidos: {len(valid_results)}")
            
            # 3. Scraping (para o primeiro resultado v√°lido)
            if valid_results:
                self.logger._log_info("3Ô∏è‚É£ Extraindo conte√∫do...")
                scraped_content = await self.scraper.scrape_content(valid_results[0].url)
                if scraped_content:
                    self.logger._log_info(f"   Conte√∫do extra√≠do: {len(scraped_content.content)} caracteres")
                    self.logger._log_info(f"   Tipo: {scraped_content.content_type.value}")
            
            # 4. Atualiza√ß√£o de conhecimento
            self.logger._log_info("4Ô∏è‚É£ Atualizando conhecimento...")
            updates = await self.knowledge_updater.process_search_results(valid_results)
            self.logger._log_info(f"   Atualiza√ß√µes criadas: {len(updates)}")
            
            if updates:
                executed = await self.knowledge_updater.execute_updates(updates)
                self.logger._log_info(f"   Atualiza√ß√µes executadas: {len(executed)}")
            
            self.logger._log_info("‚úÖ Workflow integrado conclu√≠do com sucesso!")
            
        except Exception as e:
            self.logger._log_error(f"‚ùå Erro no workflow integrado: {str(e)}")
    
    def show_final_metrics(self):
        """Exibe m√©tricas finais de todos os componentes"""
        self.logger._log_info("\nüìä M√©tricas Finais do Sistema")
        self.logger._log_info("=" * 60)
        
        # Search Engine
        se_metrics = self.search_engine.get_metrics()
        self.logger._log_info("üîç Web Search Engine:")
        self.logger._log_info(f"   Buscas: {se_metrics.total_searches}")
        self.logger._log_info(f"   Sucesso: {se_metrics.successful_searches}")
        self.logger._log_info(f"   Tempo m√©dio: {se_metrics.avg_response_time:.2f}s")
        self.logger._log_info(f"   Cache hit: {se_metrics.cache_hit_rate:.2f}")
        
        # Scraper
        sc_metrics = self.scraper.get_metrics()
        self.logger._log_info("\nüìÑ Content Scraper:")
        self.logger._log_info(f"   Scrapes: {sc_metrics.total_scrapes}")
        self.logger._log_info(f"   Sucesso: {sc_metrics.successful_scrapes}")
        self.logger._log_info(f"   Tempo m√©dio: {sc_metrics.avg_execution_time:.2f}s")
        self.logger._log_info(f"   Cache hit: {sc_metrics.cache_hit_rate:.2f}")
        
        # Validator
        val_metrics = self.validator.get_metrics()
        self.logger._log_info("\n‚úÖ Search Validator:")
        self.logger._log_info(f"   Valida√ß√µes: {val_metrics.total_validations}")
        self.logger._log_info(f"   V√°lidos: {val_metrics.valid_results}")
        self.logger._log_info(f"   Inv√°lidos: {val_metrics.invalid_results}")
        self.logger._log_info(f"   Tempo m√©dio: {val_metrics.avg_execution_time:.2f}s")
        
        # Knowledge Updater
        ku_metrics = self.knowledge_updater.get_metrics()
        self.logger._log_info("\nüîÑ Knowledge Updater:")
        self.logger._log_info(f"   Atualiza√ß√µes: {ku_metrics.total_updates}")
        self.logger._log_info(f"   Sucesso: {ku_metrics.successful_updates}")
        self.logger._log_info(f"   Taxa de sucesso: {ku_metrics.success_rate:.2f}")
        
        self.logger._log_info("\nüéâ Demonstra√ß√£o conclu√≠da com sucesso!")


async def main():
    """Fun√ß√£o principal"""
    # Verifica se a API key est√° definida
    if not os.getenv('OPENAI_API_KEY'):
        print("‚ùå Erro: OPENAI_API_KEY n√£o est√° definida")
        print("Por favor, defina a vari√°vel de ambiente:")
        print("export OPENAI_API_KEY='sua-api-key-aqui'")
        return
    
    # Cria e executa demonstra√ß√£o
    demo = WebSearchDemo()
    await demo.run_demo()


if __name__ == "__main__":
    asyncio.run(main()) 